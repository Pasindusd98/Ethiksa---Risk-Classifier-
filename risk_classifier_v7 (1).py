# -*- coding: utf-8 -*-
"""risk_classifier_v7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/134MoYxMBfCZIbEKbSwDoShFroWbtN-xP
"""

# rc_pipeline_v7_pdf_fixed_patch.py
# Requirements:
#   pip install pandas numpy sentence-transformers cross-encoder faiss-cpu scikit-learn transformers scipy pymupdf pytesseract pillow opencv-python
# Also install system tesseract if you want OCR fallback (e.g., `sudo apt install tesseract-ocr`)

import os
from pathlib import Path
import re
import json
import time
import numpy as np
import pandas as pd

# Optional OCR imports (wrapped in try/except)
try:
    import fitz  # PyMuPDF
except Exception:
    fitz = None

try:
    from PIL import Image
except Exception:
    Image = None

try:
    import pytesseract
except Exception:
    pytesseract = None

try:
    import io
except Exception:
    io = None

try:
    import cv2
except Exception:
    cv2 = None

# -------------------- CONFIG --------------------
SYN_CSV = "synthetic_queries_v3.csv"
AI_CHUNKS_CSV = "ai_act_chunks.csv"
GDPR_CHUNKS_CSV = "gdpr_chunks.csv"

# Models & index settings
RETRIEVER_MODEL = "all-mpnet-base-v2"
RERANKER_MODEL = "cross-encoder/ms-marco-MiniLM-L-6-v2"
EMBED_CACHE_DIR = Path("emb_cache")
EMBED_CACHE_DIR.mkdir(exist_ok=True)

# Retrieval / rerank params
TOP_K = 10           # used for PDF page retrieval (keeps small)
QUERY_TOP_K = 50     # larger for interactive queries to increase recall
RERANK_TOP = 6
SIM_THRESHOLD = 0.60

# PDF / OCR
OCR_ZOOM = 2.0
OCR_LANG = "eng"
OCR_CONF_THRESHOLD = 30.0

# Risk thresholds
RISK_LEVEL_THRESHOLDS = {"high": 0.7, "medium": 0.4}

OUT_DIR = Path("rc_outputs")
OUT_DIR.mkdir(exist_ok=True)

# -------------------- UTILITIES --------------------
def detect_pii(text: str):
    found = []
    if not text:
        return []
    if re.search(r"[\w\.-]+@[\w\.-]+\.\w+", text):
        found.append("email")
    if re.search(r"\b(?:\+?\d{1,3}[-.\s]?)?(?:\(?\d{2,4}\)?[-.\s]?){2,}\d{2,4}\b", text):
        found.append("phone")
    if re.search(r"\b(?:ssn|nid|nic|passport)[\s:]*[A-Za-z0-9\-]{3,}\b", text, re.I):
        found.append("id_number")
    m = re.search(r"\bmy name is ([A-Z][a-z]+)\b", text)
    if m:
        found.append("name")
    return list(set(found))

def safe_read_csv(path):
    p = Path(path)
    if not p.exists():
        print(f"[WARN] Missing CSV: {path}")
        return pd.DataFrame()
    return pd.read_csv(p).fillna("")

def choose_text_col(df):
    candidates = ["text","snippet_text","simple_question","question","source","content"]
    for c in candidates:
        if c in df.columns:
            return c
    medians = {col: df[col].astype(str).str.len().median() for col in df.columns}
    return max(medians, key=medians.get)

# -------------------- LOAD DATA --------------------
print("Loading CSVs...")
syn = safe_read_csv(SYN_CSV)
ai_chunks = safe_read_csv(AI_CHUNKS_CSV)
gdpr_chunks = safe_read_csv(GDPR_CHUNKS_CSV)

if syn.empty or (ai_chunks.empty and gdpr_chunks.empty):
    raise SystemExit("One or more required CSVs are missing or empty. Place files and re-run.")

syn_text_col = choose_text_col(syn)
syn = syn.rename(columns={syn_text_col: "simple_question"})
if "policy_id" not in syn.columns:
    raise SystemExit("synthetic CSV must have a 'policy_id' column mapping to chunk ids.")

chunks = pd.concat([ai_chunks, gdpr_chunks], ignore_index=True).fillna("")
chunk_text_col = choose_text_col(chunks)
chunks = chunks.rename(columns={chunk_text_col: "snippet_text"})
if "policy_id" not in chunks.columns:
    raise SystemExit("chunk CSVs must contain 'policy_id' column.")
if "risk_category" not in chunks.columns:
    chunks["risk_category"] = chunks.get("risk_category", "")

doc_store = {}
for _, r in chunks.iterrows():
    pid = str(r["policy_id"])
    doc_store[pid] = {
        "snippet_text": str(r["snippet_text"]),
        "risk_category": str(r.get("risk_category","")),
        "base_id": "_".join(pid.split("_")[:4]) if "_" in pid else pid
    }

print(f"Loaded {len(syn)} synthetic queries and {len(doc_store)} chunks.")

# -------------------- EMBEDDERS / INDEX --------------------
from sentence_transformers import SentenceTransformer
embedder = SentenceTransformer(RETRIEVER_MODEL)

syn_cache = EMBED_CACHE_DIR / "syn_emb.npy"
syn_texts = syn["simple_question"].astype(str).tolist()

if syn_cache.exists():
    syn_emb = np.load(syn_cache)
    print("Loaded synthetic embeddings from cache.")
else:
    print("Encoding synthetic queries...")
    syn_emb = embedder.encode(syn_texts, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=True)
    np.save(syn_cache, syn_emb)

# build FAISS or sklearn fallback
USE_FAISS = False
try:
    import faiss
    USE_FAISS = True
    dim = syn_emb.shape[1]
    faiss_index = faiss.IndexFlatIP(dim)
    faiss_index.add(syn_emb.astype(np.float32))
    print("Built FAISS index for synthetic queries.")
except Exception:
    from sklearn.neighbors import NearestNeighbors
    nn = NearestNeighbors(n_neighbors=min(max(QUERY_TOP_K, TOP_K), len(syn_emb)), metric="cosine").fit(syn_emb)
    print("FAISS not available â€” using sklearn NearestNeighbors for retrieval.")

# cross-encoder
from sentence_transformers.cross_encoder import CrossEncoder
reranker = CrossEncoder(RERANKER_MODEL)
print("Loaded cross-encoder reranker.")

# toxicity classifier
from transformers import pipeline
print("Loading toxicity classifier (may take a moment)...")
# some HF versions: top_k=None returns list/dicts correctly
toxicity_clf = pipeline("text-classification", model="unitary/toxic-bert", top_k=None)

TOXIC_LEXICON = [
    "fuck","die","kill","bomb","terror","i hate","immigrant","immigrants",
    "nigger","bitch","slur","go die","go to hell","fascist","kill yourself"
]

import unicodedata
def normalize_text_for_lexicon(t: str):
    t = str(t).lower()
    t = re.sub(r'(.)\1{2,}', r'\1\1', t)
    t = t.replace('4','a').replace('3','e').replace('1','i').replace('0','o').replace('5','s')
    t = unicodedata.normalize('NFKD', t)
    return t

def lexicon_hits(text: str):
    t = normalize_text_for_lexicon(text)
    return [w for w in TOXIC_LEXICON if w in t]

def detect_toxicity_spans(text: str, sentence_split_regex=r'(?<=[.!?\n])\s+'):
    if not text or not text.strip():
        return [], {"notice":"green","message":"No toxicity/hate/threat detected with current detectors."}
    sentences = [s.strip() for s in re.split(sentence_split_regex, text) if s.strip()]
    spans = []
    for i, s in enumerate(sentences):
        lx = lexicon_hits(s)
        try:
            out = toxicity_clf(s[:1000])
        except Exception:
            out = []
        per_label = {}
        # Several HF pipeline shapes - handle robustly
        if out and isinstance(out, list) and isinstance(out[0], dict) and 'label' in out[0]:
            # list of dicts form
            for d in out:
                per_label[d['label'].lower()] = float(d.get('score', 0.0))
        elif out and isinstance(out[0], list):
            # nested list form
            for d in out[0]:
                per_label[d['label'].lower()] = float(d.get('score', 0.0))
        else:
            # fallback: attempt to parse
            try:
                for d in out:
                    if isinstance(d, dict) and 'label' in d:
                        per_label[d['label'].lower()] = float(d.get('score', 0.0))
            except Exception:
                per_label = {}

        toxic_labels = ['toxic','severe_toxicity','threat','insult','identity_hate','obscene']
        ml_score = max([per_label.get(lbl, 0.0) for lbl in toxic_labels]) if per_label else 0.0

        categories = []
        if per_label.get('threat', 0.0) > 0.35 or re.search(r'\b(kill|bomb|die|harm|destroy)\b', s, re.I):
            categories.append('Threat')
        if per_label.get('identity_hate', 0.0) > 0.25 or any(w in ' '.join(lx) for w in ['immigrant','nigger','fascist']):
            categories.append('Hate')
        if per_label.get('insult', 0.0) > 0.25 or per_label.get('obscene', 0.0) > 0.25 or lx:
            categories.append('Toxic/Profanity')

        spans.append({
            'idx': i,
            'text': s,
            'lex_hits': lx,
            'ml_score': ml_score,
            'per_label': per_label,
            'categories': list(set(categories))
        })

    all_cats = set(c for sp in spans for c in sp['categories'])
    doc_toxic_score = max([sp['ml_score'] for sp in spans]) if spans else 0.0
    if not all_cats:
        summary = {"notice":"green","message":"No toxicity/hate/threat detected with current detectors.","doc_toxic_score": doc_toxic_score}
    else:
        summary = {"notice":"red","message": f"Detected categories: {', '.join(sorted(all_cats))}", "categories": sorted(all_cats), "doc_toxic_score": doc_toxic_score}
    return spans, summary

# -------------------- Retrieval helpers --------------------
def retrieve_candidate_chunk_ids(query, top_k=TOP_K):
    """
    Returns a list of policy_ids (from synthetic query mapping) most similar to `query`.
    top_k: how many synthetic queries to fetch before de-duplicating policy ids.
    """
    q_emb = embedder.encode([query], convert_to_numpy=True, normalize_embeddings=True)
    # if faiss available, use it
    if USE_FAISS:
        D, I = faiss_index.search(q_emb.astype(np.float32), top_k)
        indices = I[0].tolist()
    else:
        n_neigh = min(top_k, len(syn_emb))
        # sklearn kneighbors expects 2d q_emb
        dists, indices = nn.kneighbors(q_emb, n_neighbors=n_neigh)
        indices = indices[0].tolist()
    policy_ids = syn.iloc[indices]["policy_id"].astype(str).tolist()
    seen = set(); uniq = []
    for pid in policy_ids:
        if pid not in seen:
            seen.add(pid); uniq.append(pid)
    return uniq

from scipy.special import expit
print("Encoding chunk embeddings for similarity signals...")
chunk_ids = list(doc_store.keys())
chunk_texts = [doc_store[cid]["snippet_text"] for cid in chunk_ids]
chunk_emb_cache = EMBED_CACHE_DIR / "chunk_emb.npy"
if chunk_emb_cache.exists():
    chunk_embs = np.load(chunk_emb_cache)
else:
    chunk_embs = embedder.encode(chunk_texts, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=True)
    np.save(chunk_emb_cache, chunk_embs)
chunkid_to_idx = {cid: i for i, cid in enumerate(chunk_ids)}

def rerank_chunks_with_probs(query, chunk_ids, top_n=RERANK_TOP):
    """
    chunk_ids: list of policy_id (these are keys in doc_store)
    returns top_n results with fields including combined_score (0..1)
    """
    if not chunk_ids:
        return []

    pairs = []
    valid_ids = []
    for pid in chunk_ids:
        info = doc_store.get(pid)
        if info:
            pairs.append([query, info["snippet_text"]])
            valid_ids.append(pid)

    if not pairs:
        return []

    # cross-encoder logits (may be 1D or 2D depending on model/versions)
    logits = reranker.predict(pairs, show_progress_bar=False)
    logits = np.array(logits).squeeze()
    # ensure shape (N,)
    if logits.ndim > 1:
        logits = logits.reshape(-1)

    # map to (0,1)
    reranker_probs = expit(logits)

    # query embedding as 1D vector
    q_emb = embedder.encode([query], convert_to_numpy=True, normalize_embeddings=True)
    q_vec = q_emb[0] if hasattr(q_emb, 'shape') and len(q_emb.shape) > 1 else q_emb

    cos_sims = []
    for pid in valid_ids:
        idx = chunkid_to_idx.get(pid)
        if idx is None:
            cos_sims.append(0.0)
        else:
            # dot product between normalized vectors -> cosine similarity in [-1,1]
            cos = float(np.dot(q_vec, chunk_embs[idx]))
            # scale cosine from [-1,1] to [0,1] to make combination with reranker_probs safer
            cos_scaled = (cos + 1.0) / 2.0
            cos_sims.append(cos_scaled)

    alpha = 0.75
    combined_scores = alpha * reranker_probs + (1.0 - alpha) * np.array(cos_sims)

    results = []
    for i, (pid, prob, cosv, comb) in enumerate(zip(valid_ids, reranker_probs, cos_sims, combined_scores)):
        info = doc_store[pid]
        results.append({
            "policy_id": pid,
            "base_id": info.get("base_id"),
            "risk_category": info.get("risk_category"),
            "snippet_text": info.get("snippet_text"),
            "reranker_logit": float(logits[i]) if i < len(logits) else 0.0,
            "reranker_prob": float(prob) if i < len(reranker_probs) else 0.0,
            "cos_sim": float(cosv),
            "combined_score": float(comb)
        })

    results.sort(key=lambda x: x["combined_score"], reverse=True)
    return results[:top_n]

# -------------------- PDF helpers --------------------
def extract_text_from_pdf(pdf_path, zoom=OCR_ZOOM, ocr_lang=OCR_LANG, ocr_conf_threshold=OCR_CONF_THRESHOLD):
    pages = []
    if fitz is None:
        raise RuntimeError("PyMuPDF (fitz) is required for PDF extraction. Install pymupdf.")
    doc = fitz.open(pdf_path)
    for i in range(len(doc)):
        page = doc.load_page(i)
        txt = page.get_text("text").strip()
        page_info = {"page_num": i, "text": txt, "is_selectable": bool(txt), "ocr_boxes": None}
        if not txt:
            # OCR fallback
            mat = fitz.Matrix(zoom, zoom)
            pix = page.get_pixmap(matrix=mat, alpha=False)
            img = Image.open(io.BytesIO(pix.tobytes()))
            arr = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)
            gray = cv2.cvtColor(arr, cv2.COLOR_BGR2GRAY)
            gray = cv2.fastNlMeansDenoising(gray, None, 10, 7, 21)
            _, th = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)

            if pytesseract is None:
                raise RuntimeError("pytesseract is required for OCR fallback. Install pytesseract and system tesseract.")
            ocr_data = pytesseract.image_to_data(th, lang=ocr_lang, output_type=pytesseract.Output.DICT)
            words = []
            text_acc = []
            n_boxes = len(ocr_data['text'])
            for idx in range(n_boxes):
                w = str(ocr_data['text'][idx]).strip()
                conf_raw = ocr_data['conf'][idx]
                try:
                    conf = float(conf_raw)
                except Exception:
                    try:
                        conf = float(conf_raw) if conf_raw else -1.0
                    except Exception:
                        conf = -1.0
                if w and conf >= ocr_conf_threshold:
                    left = int(ocr_data['left'][idx])
                    top = int(ocr_data['top'][idx])
                    width = int(ocr_data['width'][idx])
                    height = int(ocr_data['height'][idx])
                    words.append({'word': w, 'left': left, 'top': top, 'width': width, 'height': height, 'conf': conf})
                    text_acc.append(w)
            ocr_text = " ".join(text_acc).strip()
            page_info['text'] = ocr_text
            page_info['is_selectable'] = False
            page_info['ocr_boxes'] = words
        pages.append(page_info)
    doc.close()
    return pages

# -------------------- Scoring / aggregation --------------------
def score_to_severity(score: float):
    if score >= RISK_LEVEL_THRESHOLDS["high"]:
        return "High"
    if score >= RISK_LEVEL_THRESHOLDS["medium"]:
        return "Medium"
    return "Low"

def aggregate_document_risk(violations_list, safety_doc_score):
    if not violations_list:
        if safety_doc_score >= RISK_LEVEL_THRESHOLDS["high"]:
            return "High"
        if safety_doc_score >= RISK_LEVEL_THRESHOLDS["medium"]:
            return "Medium"
        return "Low"

    severities = [v.get("violation_severity","Low") for v in violations_list]
    counts = {"High":0,"Medium":0,"Low":0}
    for s in severities:
        counts[s] = counts.get(s, 0) + 1

    if counts["High"] > 0:
        return "High"
    total = counts["High"] + counts["Medium"] + counts["Low"]
    if counts["Medium"] > total/2:
        return "Medium"
    if safety_doc_score >= RISK_LEVEL_THRESHOLDS["high"]:
        return "High"
    if safety_doc_score >= RISK_LEVEL_THRESHOLDS["medium"]:
        return "Medium"
    return "Low"

# -------------------- CLASSIFIERS --------------------
def classify_pdf(pdf_path, run_per_page=True, top_k=TOP_K):
    start_time = time.time()
    pages = extract_text_from_pdf(pdf_path)
    full_text = "\n\n".join([p['text'] for p in pages if p['text']])

    pii_doc = detect_pii(full_text)
    spans_doc, safety_summary_doc = detect_toxicity_spans(full_text)
    doc_toxic_score = float(safety_summary_doc.get("doc_toxic_score", 0.0) if isinstance(safety_summary_doc, dict) else 0.0)

    candidate_ids_doc = retrieve_candidate_chunk_ids(full_text, top_k=top_k)
    # if no candidate ids (rare), use all chunk_ids
    if not candidate_ids_doc:
        candidate_ids_doc = chunk_ids.copy()

    reranked_doc = rerank_chunks_with_probs(full_text, candidate_ids_doc, top_n=RERANK_TOP)

    violations = {}
    def record_match(match, page_num=None, context_text=None):
        pid = match.get("policy_id")
        if not pid:
            return
        cur = violations.get(pid)
        score = float(match.get("combined_score", 0.0))
        if cur is None:
            violations[pid] = {
                "policy_id": pid,
                "base_id": match.get("base_id"),
                "risk_category": match.get("risk_category"),
                "best_score": score,
                "occurrences": 1,
                "pages": [page_num] if page_num is not None else [],
                "contexts": [context_text] if context_text else [match.get("snippet_text","")]
            }
        else:
            cur["occurrences"] += 1
            if score > cur["best_score"]:
                cur["best_score"] = score
            if page_num is not None and page_num not in cur["pages"]:
                cur["pages"].append(page_num)
            if context_text:
                cur["contexts"].append(context_text)

    for m in reranked_doc:
        record_match(m, page_num=None, context_text=m.get("snippet_text"))

    page_evidence = []
    if run_per_page:
        for p in pages:
            text = p.get('text','').strip()
            if not text:
                page_evidence.append({
                    "page_num": p['page_num'],
                    "is_selectable": p['is_selectable'],
                    "pii": [],
                    "safety_summary": {"notice":"green","message":"No text"},
                    "top_matches": []
                })
                continue
            pii_page = detect_pii(text)
            spans_page, safety_summary_page = detect_toxicity_spans(text)
            cand = retrieve_candidate_chunk_ids(text, top_k=top_k)
            if not cand:
                cand = chunk_ids.copy()
            reranked_page = rerank_chunks_with_probs(text, cand, top_n=RERANK_TOP)
            for m in reranked_page:
                record_match(m, page_num=p['page_num'], context_text=text[:400])
            page_evidence.append({
                "page_num": p['page_num'],
                "is_selectable": p['is_selectable'],
                "pii": pii_page,
                "safety_summary": safety_summary_page,
                "top_matches": reranked_page,
                "ocr_boxes": p.get('ocr_boxes')
            })

    violations_list = []
    for pid, info in violations.items():
        violations_list.append({
            "policy_id": pid,
            "base_id": info.get("base_id"),
            "risk_category": info.get("risk_category"),
            "best_score": float(info.get("best_score",0.0)),
            "occurrences": info.get("occurrences",0),
            "pages": info.get("pages",[]),
            "contexts": info.get("contexts",[])
        })
    violations_list.sort(key=lambda x: x["best_score"], reverse=True)
    for v in violations_list:
        v["violation_severity"] = score_to_severity(v["best_score"])

    highest_policy_score = violations_list[0]["best_score"] if violations_list else 0.0
    overall_confidence = max(highest_policy_score, doc_toxic_score)
    overall_risk_level = aggregate_document_risk(violations_list, doc_toxic_score)
    violations_above_threshold = [v for v in violations_list if v["best_score"] >= SIM_THRESHOLD]

    counts = {"High":0,"Medium":0,"Low":0}
    for v in violations_list:
        counts[v.get("violation_severity","Low")] += 1
    guideline_lines = []
    guideline_lines.append(f"Detected {len(violations_list)} potential policy violations: {counts['High']} High, {counts['Medium']} Medium, {counts['Low']} Low.")
    guideline_lines.append(f"Document toxicity score: {doc_toxic_score:.2f}.")
    guideline_lines.append("Aggregation rule: if any violation is High -> document is High risk; else if majority of violations are Medium -> Medium risk; else Low risk.")
    guideline_lines.append(f"Overall decision: {overall_risk_level} (confidence {overall_confidence:.2f}).")
    sample_examples = []
    for v in violations_list[:3]:
        sample_examples.append(f"{v['policy_id']} ({v['violation_severity']}, score {v['best_score']:.2f}) - pages {v['pages']}")
    if sample_examples:
        guideline_lines.append("Examples: " + " | ".join(sample_examples))
    guideline_text = " ".join(guideline_lines)

    out = {
        "source": str(pdf_path),
        "num_pages": len(pages),
        "violations_all": violations_list,
        "violations_above_threshold": violations_above_threshold,
        "num_violations": len(violations_list),
        "num_violations_above_threshold": len(violations_above_threshold),
        "overall_confidence": overall_confidence,
        "risk_level": overall_risk_level,
        "pii_detected_doc": pii_doc,
        "safety_summary_doc": safety_summary_doc,
        "page_evidence": page_evidence,
        "guideline": {
            "text": guideline_text,
            "counts": counts,
            "aggregation_rule": "any-High -> High; else majority-Medium -> Medium; else Low",
            "examples": sample_examples
        },
        "duration_s": time.time() - start_time
    }

    ts = int(time.time()*1000)
    out_fname = OUT_DIR / f"pdf_match_{ts}.json"
    with open(out_fname, 'w', encoding='utf-8') as f:
        json.dump(out, f, indent=2, ensure_ascii=False)
    print("Saved PDF match ->", out_fname)
    return out

def get_violated_act_name(base_id):
    if not base_id:
        return None
    if base_id.startswith("EU_AI_Act"):
        return "EU AI Act"
    if base_id.startswith("GDPR"):
        return "GDPR"
    return base_id

def match_query(query: str, query_top_k=QUERY_TOP_K):
    start = time.time()
    query = str(query).strip()
    if not query:
        return {"query": query, "decision": "no_input"}

    pii_detected = detect_pii(query)
    spans, safety_summary = detect_toxicity_spans(query)
    doc_toxic_score = float(safety_summary.get("doc_toxic_score", 0.0) if isinstance(safety_summary, dict) else 0.0)

    candidate_ids = retrieve_candidate_chunk_ids(query, top_k=query_top_k)
    # fallback: if no candidate ids, use all chunk ids
    if not candidate_ids:
        candidate_ids = chunk_ids.copy()

    reranked_results = rerank_chunks_with_probs(query, candidate_ids, top_n=RERANK_TOP)

    result = {
        "query": query,
        "violated_act": None,
        "policy_id": None,
        "risk_category": "Low",
        "confidence": 0.0,
        "pii_detected": pii_detected,
        "safety_summary": safety_summary,
        "reason": "No direct policy violation detected.",
        "duration_s": time.time() - start,
        "decision": "Low"
    }

    if reranked_results:
        top_match = reranked_results[0]
        conf = float(top_match.get("combined_score", 0.0))
        result["confidence"] = conf
        result["policy_id"] = top_match.get("policy_id")
        result["violated_act"] = get_violated_act_name(top_match.get("base_id"))
        result["risk_category"] = top_match.get("risk_category")
        result["reason"] = top_match.get("snippet_text")
        # decision based on thresholds
        if conf >= SIM_THRESHOLD:
            result["decision"] = score_to_severity(conf)
        else:
            # if doc toxicity is higher, escalate
            result["decision"] = score_to_severity(max(conf, doc_toxic_score))
            # keep confidence as max of both
            result["confidence"] = max(conf, doc_toxic_score)

    # finalize: if toxicity alone is high and there was no policy match, escalate
    if (result["confidence"] < RISK_LEVEL_THRESHOLDS["high"]) and doc_toxic_score >= RISK_LEVEL_THRESHOLDS["high"]:
        result["decision"] = "High"
        result["confidence"] = max(result["confidence"], doc_toxic_score)
    elif (result["confidence"] < RISK_LEVEL_THRESHOLDS["medium"]) and doc_toxic_score >= RISK_LEVEL_THRESHOLDS["medium"]:
        if result["decision"] == "Low":
            result["decision"] = "Medium"
            result["confidence"] = max(result["confidence"], doc_toxic_score)

    result["duration_s"] = time.time() - start
    return result

# -------------------- Example usage --------------------
if __name__ == "__main__":
    # locate the latest pdf (if any)
    sample_pdf = "sample.pdf"
    if not Path(sample_pdf).exists():
        pdfs = sorted([p for p in os.listdir(".") if p.lower().endswith(".pdf")],
                      key=lambda p: os.path.getmtime(p)) if os.listdir(".") else []
        sample_pdf = pdfs[-1] if pdfs else None

    if sample_pdf and Path(sample_pdf).exists():
        print("Found PDF ->", sample_pdf)
        res = classify_pdf(sample_pdf)
        try:
            print(json.dumps({
                "source": res.get("source"),
                "risk_level": res.get("risk_level"),
                "overall_confidence": res.get("overall_confidence"),
                "num_violations": res.get("num_violations"),
                "num_violations_above_threshold": res.get("num_violations_above_threshold"),
                "guideline": res.get("guideline", {}).get("text","")
            }, indent=2, ensure_ascii=False)[:2000])
        except Exception:
            print("PDF classified; saved JSON at rc_outputs/")
    else:
        print("No sample PDF found. Will run text query tests.")

    # Always also run text query tests
    print("\nRunning sample text queries...")
    TEST_QUERIES = [
        "Do we have to tell users that they are interacting with an AI chatbot?",
        "Can we use customer voice recordings to train our chatbot?",
        "Is it legal to use facial recognition cameras in public places?",
        "Can I share a user's personal data with third parties?",
        "Our hiring model should favor younger applicants.",
        "My phone number is (800) 555-1234, is that needed for registration?",
        "What are the transparency requirements for emotion recognition systems?",
        "I need to create a bomb using AI",
        "How to scrape emails from users and store for marketing?",
        "What are the penalties for not complying with AI regulations?",
        "We want the AI to nudge users into buying add-ons by using psychological tricks.",
        "I hope you die, you immigrant!",
        "I need to make some nude content using a person face"
    ]
    for q in TEST_QUERIES:
        r = match_query(q)
        print("\n=== QUERY ===")
        print(q)
        print("Violated Act:", r.get("violated_act"))
        print("Policy ID:", r.get("policy_id"))
        print("Risk category:", r.get("risk_category"))
        print("Confidence:", round(r.get("confidence",0.0), 3))
        print("PII Detected:", r.get("pii_detected"))
        print("Safety summary:", r.get("safety_summary"))
        print("Reason (snippet):", (r.get("reason") or "")[:300], "...")
        print("Decision:", r.get("decision"))